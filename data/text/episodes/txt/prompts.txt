
# prompts.txt

### （一）BERT

1. **路径优化**：
   - 从 `file_path` 取和存 BERT 标记好的文件，确保文件名不与原 BERT 处理的文件名重名。
   - 文件存储路径为 `../data/ft_data/labelled`，合并为 "all_labelled.txt" 存在同一个文件夹。
   - 合并时根据数据集抽取贴吧名字，例如：
     - `'tp_yuanshen_tp_ft_trunc500000_labeled'` 来自 `'yuanshen'`，贴吧名为 `'yuanshen'`。
     - `'pc_aiouniya_pc_ft_trunc0_labeled'` 来自 `'aiouniya'`，贴吧名为 `'aiouniya'`。
   - 数据来源 `'source'` 的抽取规则：
     - `'tp_yuanshen_tp_ft_trunc500000_labeled'` 对应的是 `'tp'`，来源为 `'tp'`。
     - `'pc_aiouniya_pc_ft_trunc0_labeled'` 对应的是 `'pc'`，来源为 `'pc'`。
   - 抽取后的两列为 `tieba_name` 和 `source`，加入合并后的数据集。
   - 最终生成的文件列数与 `all_labelled.txt` 一致，共六列，行数会不同。

2. **工程改进**：
   - 当前有三个主要任务流程：
     1. BERT 的数据初始化。
     2. 加入断点处理的逻辑。
     3. 自迭代标签分布未见明显变化，需优化自迭代超参数。

   - 建议：
     - 增加 `FastText` 有监督训练和自迭代。
     - 优化数据加载：针对仅有8GB内存的设备，将 `all_labelled.txt` 转换为 `Dask` 格式，压缩数据加载到内存。
     - 参数微调：
       1. 动态调整 NVIDIA 4070 的显存调度。
       2. 部署模型剪枝和量化加速 NSP 模型推理。
       3. 调整 BERT NSP 模型学习率，混合使用 `MinMaxScaler` 和 `GradScaler`。

3. **人工标注**：
   - 生成结果可视化。
   - 现有两个已标注文件，各200行左右。
   - 用训练完成的 `FastText` 生成第七列 `related_ft`（区间值为[0,1]）。
   - 主观判断加入第八列 `related_classes_ft`。
   - 最终表格列包括：`tieba_name`, `source`, `ask_content`, `answer_content`, `related`, `related_classes`。

### （二）FastText

1. **有监督训练及自迭代**：
   - 确保 BERT 标记完成的数据可被 `FastText` 识别。
   - 每轮自迭代中，随机采样数据对进行负样本生成。
   - 将新生成的数据与原数据集一起训练下一轮 `FastText` 模型。

2. **分词**：
   - 没有对 `ask_content` 和 `answer_content` 进行分词，每轮迭代数据量未变。
   - 评估分词质量，结合多种 NLP 指标优化分词方案。

3. **作图**：
   - 使用回归值分类，将相关系数的回归值划分为 0 到 1 之间的十个区间（步长为 0.1）。
   - 柱状图显示样本量，每个区间样本使用不同颜色区分。

4. **自迭代作图**：
   - 横轴为贴吧名，纵轴为最大标签样本数量。
   - 保存两极化最大的一轮和最后一轮自迭代结果至 `logs/` 文件夹。

5. **数据样本留存率**：
   - 计算并显示数据样本留存率，如初始数据7142856行，留存数据4285719行，则留存率为 `4285719/7142856`。

### （三）LCCC 数据集

1. **数据清洗流程**：
   - 广告和无关内容过滤。
   - 移除 URL 和平台标记，确保文本仅含对话内容。
   - 检测并合并重复对话，清除异常对话。
   - 手动标注提高数据集准确性。
